### 使用kubeadm搭建kubernetes
> 文章全部看完后再安装#### 1.升级内核  
先升级成最新的内核，详见之前文章
#### 2.配置yum源，安装docker-ce和kubeadm
配置aliyun的docker-ce和kubernetes源
```shell
cd /etc/yum.repos.d
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

# vim kubernetes.repo
[kubernetes]
name=kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
gpgcheck=0
enable=1
```
配置好yum源之后，使用`yum clean all;yum repolist`来显示下已配置的源，看下是否生效。
```shell
源标识                                       源名称                                                                       状态
docker-ce-stable/x86_64                      Docker CE Stable - x86_64                                                        32
elrepo                                       ELRepo.org Community Enterprise Linux Repository - el7                           97
epel/7/x86_64                                EPEL for redhat/centos 7 - x86_64                                            12,917
extras/7/x86_64                              Qcloud centos extras - x86_64                                                   364
kubernetes                                   kubernetes                                                                      311
os/7/x86_64                                  Qcloud centos os - x86_64                                                    10,019
updates/7/x86_64                             Qcloud centos updates - x86_64                                                1,067
repolist: 24,807
```
安装docker-ce和kubeadm，kubeadm依赖kubectl、kubelet和kubernetes-cni，以上都会被安装。
```shell
yum install docker-ce
yum install kubeadm
```
安装好之后启动docker和kubelet（不然在kubeadm init的时候会告警）
```shell
systemctl enable docker.service
systemctl start docker.service

systemctl enable kubelet.service
systemctl start kubelet.service
```
#### 3.部署kubernetes的master节点
通过配置文件来进行kubernetes master的部署
```shell
# cat kubeadm.yaml
apiVersion: kubeadm.k8s.io/v1beta1
kind: InitConfiguration
controllerManagerExtraArgs:
  horizontal-pod-autoscaler-use-rest-clients: "true"
  horizontal-pod-autoscaler-sync-period: "10s"
  node-monitor-grace-period: "10s"
apiServerExtraArgs:
  runtime-config: "api/all=true"
kubernetesVersion: "stable-1.13"
```
使用配置文件来部署，可以先使用`kubeadm config print init-defaults`来查看当前kubeadm版本的默认配置。
```shell
kubeadm config print init-defaults
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: l-kubernetes-server1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: ""
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.13.0
networking:
  dnsDomain: cluster.local
  podSubnet: ""
  serviceSubnet: 10.96.0.0/12
scheduler: {}
```
可以从上面默认配置看出，apiVersion: kubeadm.k8s.io/v1beta1，kind: InitConfiguration，kubernetesVersion: v1.13.0等信息。
然后使用`kubeadm init --config kubeadm.yaml`来启动kubernetes master节点。但是会遇到镜像拉取问题
```shell
# kubeadm init --config kubeadm.yaml
W0219 16:48:46.120089    2618 strict.go:54] error unmarshaling configuration schema.GroupVersionKind{Group:"kubeadm.k8s.io", Version:"v1beta1", Kind:"InitConfiguration"}: error unmarshaling JSON: while decoding JSON: json: unknown field "apiServerExtraArgs"
[init] Using Kubernetes version: v1.13.3
[preflight] Running pre-flight checks
	[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.09.2. Latest validated version: 18.06
	[WARNING Hostname]: hostname "l-kubernetes-server1" could not be reached
	[WARNING Hostname]: hostname "l-kubernetes-server1": lookup l-kubernetes-server1 on 183.60.82.98:53: read udp 10.115.0.11:59023->183.60.82.98:53: i/o timeout
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-controller-manager:v1.13.3: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-scheduler:v1.13.3: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-proxy:v1.13.3: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/pause:3.1: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/etcd:3.2.24: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/coredns:1.2.6: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
```
从上面可以看出，docker拉去k8s.gcr.io的镜像失败，原因是因为k8s.gcr.io在谷歌云上，国内访问不了。
解决方法：
1. 有一个哥们写了同步工具[gcrsync](https://github.com/mritd/gcrsync)，通过[Travis CI](https://travis-ci.org/mritd/gcrsync)每天自动运行，将gcr.io中镜像同步到docker hub中。**目前对于一个gcr.io下的镜像，可以直接替换为gcrxio用户名，然后从 Docker Hub 直接拉取**，以下为一个示例:
```shell
/# 原始命令/
docker pull k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0

/# 使用同步仓库/
docker pull gcrxio/kubernetes-dashboard-amd64:v1.10.0
```
通过以上可以下载合适的版本，缺点就是需要从国外的docker hub下载，速度比较慢。
2. 为了解决速度比较慢的问题，咱们也可以自己同步仓库。在阿里云或者腾讯云中有容器镜像服务，可以自己创建仓库，拉取gcr.io的镜像，具体操作方法：[使用国内云平台同步GCR中的镜像](https://github.com/Feng-Xu/feng-xu.github.io/blob/master/docker/9.%E4%BD%BF%E7%94%A8%E5%9B%BD%E5%86%85%E4%BA%91%E5%B9%B3%E5%8F%B0%E5%90%8C%E6%AD%A5GCR%E4%B8%AD%E7%9A%84%E9%95%9C%E5%83%8F.md)
将镜像预先下载到本地后，最后执行命令：
```shell
[root@l-kubernetes-server1 kubernetes]# kubeadm init --config kubeadm.yaml
W0219 17:24:38.834978   16048 strict.go:54] error unmarshaling configuration schema.GroupVersionKind{Group:"kubeadm.k8s.io", Version:"v1beta1", Kind:"InitConfiguration"}: error unmarshaling JSON: while decoding JSON: json: unknown field "apiServerExtraArgs"
[init] Using Kubernetes version: v1.13.3
[preflight] Running pre-flight checks
	[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.09.2. Latest validated version: 18.06
	[WARNING Hostname]: hostname "l-kubernetes-server1" could not be reached
	[WARNING Hostname]: hostname "l-kubernetes-server1": lookup l-kubernetes-server1 on 183.60.83.19:53: read udp 10.115.0.11:43043->183.60.83.19:53: i/o timeout
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [l-kubernetes-server1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.115.0.11]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [l-kubernetes-server1 localhost] and IPs [10.115.0.11 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [l-kubernetes-server1 localhost] and IPs [10.115.0.11 127.0.0.1 ::1]
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.
[apiclient] All control plane components are healthy after 54.002002 seconds
[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster
[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "l-kubernetes-server1" as an annotation
[mark-control-plane] Marking the node l-kubernetes-server1 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node l-kubernetes-server1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: p6a1l1.qcuwpgpiacey4ssq
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 10.115.0.11:6443 --token p6a1l1.qcuwpgpiacey4ssq --discovery-token-ca-cert-hash sha256:f7e5182b29c8de469ac9838bcffce6f2042029acc5830cbd0648b88bd1c254c8
```
出现了以上信息，就表示完成了kubernetes Master的部署了。**但是上面的信息中有error：error unmarshaling JSON: while decoding JSON: json: unknown field "apiServerExtraArgs"**
从上面的error信息中可以看出单独的配置文件kubeadm.yaml中关于“extraArgs”参数没起作用。查询官方文档，从上面init-defaults默认配置中看出，当前使用v1beta1版本（之前旧的版本是v1alpha1/v1alpha2/v1alpha3），从[v1beta1 API](https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta1#hdr-Basics)中得到v1beta1版本的配置文件是BETA级别的，是向GA的一大步，不再是alpha版本了。
从[v1beta1 API](https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta1#hdr-Basics)中得到在ClusterConfiguration中关于“*extraArgs”的参数取消了，变成了“apiServer”，“controllerManager”，“scheduler”的子结构。这也是导致上面的kubeadm init错误的原因。
在v1beta1的版本中，kubeadm的配置文件包含多种配置块，每个配置块之间使用“—-”分隔开。配置块如下：
```
apiVersion: kubeadm.k8s.io/v1beta1
kind: InitConfiguration

apiVersion: kubeadm.k8s.io/v1beta1
kind: ClusterConfiguration

apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration

apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration

apiVersion: kubeadm.k8s.io/v1beta1
kind: JoinConfiguration
```
可以使用以下命令查看“init”和“join”的默认配置
```
kubeadm config print init-defaults
kubeadm config print join-defaults
```
以上这些配置如果没有则使用默认配置，如果kubeadm init指定了配置文件，则会覆盖默认配置。
kubeadm init配置文件包含四种类型：InitConfiguration，ClusterConfiguration，KubeProxyConfiguration，KubeletConfiguration，但是只有InitConfiguration和ClusterConfiguration是强制必需的。以下是官网v1beta1版本中一个kubeadm init的YAML配置文件的例子：
```
apiVersion: kubeadm.k8s.io/v1beta1
kind: InitConfiguration
bootstrapTokens:
- token: "9a08jv.c0izixklcxtmnze7"
  description: "kubeadm bootstrap token"
  ttl: "24h"
- token: "783bde.3f89s0fje9f38fhf"
  description: "another bootstrap token"
  usages:
  - authentication
  - signing
  groups:
  - system:bootstrappers:kubeadm:default-node-token
nodeRegistration:
  name: "ec2-10-100-0-1"
  criSocket: "/var/run/dockershim.sock"
  taints:
  - key: "kubeadmNode"
    value: "master"
    effect: "NoSchedule"
  kubeletExtraArgs:
    cgroupDriver: "cgroupfs"
localAPIEndpoint:
  advertiseAddress: "10.100.0.1"
  bindPort: 6443
---
apiVersion: kubeadm.k8s.io/v1beta1
kind: ClusterConfiguration
etcd:
  # one of local or external
  local:
    imageRepository: "k8s.gcr.io"
    imageTag: "3.2.24"
    dataDir: "/var/lib/etcd"
    extraArgs:
      listen-client-urls: " [http://10.100.0.1:2379](http://10.100.0.1:2379/) "
    serverCertSANs:
    -  "ec2-10-100-0-1.compute-1.amazonaws.com"
    peerCertSANs:
    - "10.100.0.1"
  # external:
    # endpoints:
    # - "10.100.0.1:2379"
    # - "10.100.0.2:2379"
    # caFile: "/etcd/kubernetes/pki/etcd/etcd-ca.crt"
    # certFile: "/etcd/kubernetes/pki/etcd/etcd.crt"
    # keyFile: "/etcd/kubernetes/pki/etcd/etcd.key"
networking:
  serviceSubnet: "10.96.0.0/12"
  podSubnet: "10.100.0.1/24"
  dnsDomain: "cluster.local"
kubernetesVersion: "v1.12.0"
controlPlaneEndpoint: "10.100.0.1:6443"
apiServer:
  extraArgs:
    authorization-mode: "Node,RBAC"
  extraVolumes:
  - name: "some-volume"
    hostPath: "/etc/some-path"
    mountPath: "/etc/some-pod-path"
    readOnly: false
    pathType: File
  certSANs:
  - "10.100.1.1"
  - "ec2-10-100-0-1.compute-1.amazonaws.com"
  timeoutForControlPlane: 4m0s
controllerManager:
  extraArgs:
    "node-cidr-mask-size": "20"
  extraVolumes:
  - name: "some-volume"
    hostPath: "/etc/some-path"
    mountPath: "/etc/some-pod-path"
    readOnly: false
    pathType: File
scheduler:
  extraArgs:
    address: "10.100.0.1"
extraVolumes:
- name: "some-volume"
  hostPath: "/etc/some-path"
  mountPath: "/etc/some-pod-path"
  readOnly: false
  pathType: File
certificatesDir: "/etc/kubernetes/pki"
imageRepository: "k8s.gcr.io"
useHyperKubeImage: false
clusterName: "example-cluster"
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
# kubelet specific options here
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
# kube-proxy specific options here
```
Kubeadm join操作也可以使用配置文件：`kebeadm join --config xxxx.yaml`，具体配置文件类型：
```
apiVersion: kubeadm.k8s.io/v1beta1
kind: JoinConfiguration
NodeRegistration:
  ...
APIEndpoint:
  ...
```
**基于官网的kubead init的配置文件和API说明，我们发现，在最开始的kubeadm.yaml配置文件内容有问题，与v1beta1版本不相符，所以会报出参数不识别的错误： json: unknown field "apiServerExtraArgs"**，以下对最开始的kubeadm.yaml配置文件进行修改：
```
apiVersion: kubeadm.k8s.io/v1beta1
kind: InitConfiguration
---
apiVersion: kubeadm.k8s.io/v1beta1
kind: ClusterConfiguration
apiServer:
  extraArgs:
    runtime-config: "api/all=true"
controllerManager:
  extraArgs:
    horizontal-pod-autoscaler-use-rest-clients: "true"
    horizontal-pod-autoscaler-sync-period: "10s"
    node-monitor-grace-period: "10s"
kubernetesVersion: "v1.13.3"
```
然后再去执行`kubeadm init --config kubeadm.yaml`发现最开始的报错没有了。
```bash
# kubeadm init --config kubeadm.yaml
[init] Using Kubernetes version: v1.13.3
[preflight] Running pre-flight checks
	[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.09.2. Latest validated version: 18.06
	[WARNING Hostname]: hostname "l-kubernetes-server1" could not be reached
	[WARNING Hostname]: hostname "l-kubernetes-server1": lookup l-kubernetes-server1 on 10.106.2.16:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [l-kubernetes-server1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.115.0.11]
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [l-kubernetes-server1 localhost] and IPs [10.115.0.11 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [l-kubernetes-server1 localhost] and IPs [10.115.0.11 127.0.0.1 ::1]
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 20.002600 seconds
[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster
[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "l-kubernetes-server1" as an annotation
[mark-control-plane] Marking the node l-kubernetes-server1 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node l-kubernetes-server1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: eehj83.u4xgm58stlw133sn
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 10.115.0.11:6443 --token eehj83.u4xgm58stlw133sn --discovery-token-ca-cert-hash sha256:37f7023a31ffa3118375ad409e8a3a08db4b628c69ec332b2f6d22770c02777c
```
**至此，kubernetes master安装成功**
**但是，上面还有告警，[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.09.2. Latest validated version: 18.06**，从上面信息可以看到，之前安装的docker版本是18.09.2的版本，但是未经过kubernetes的验证，经过验证的最新版本是18.06，那么卸载docker-ce，重新安装docker-ce-18.06
```shell
# rpm -qa|grep docker
docker-ce-cli-18.09.2-3.el7.x86_64
docker-ce-18.09.2-3.el7.x86_64
# yum erase docker-ce-cli-18.09.2 docker-ce-18.09.2
# 查看可用的docker-ce版本
# yum list docker-ce --showduplicates
已加载插件：fastestmirror, langpacks
Loading mirror speeds from cached hostfile
 * elrepo: mirrors.tuna.tsinghua.edu.cn
可安装的软件包
docker-ce.x86_64                                                   17.03.0.ce-1.el7.centos                                                    docker-ce-stable
docker-ce.x86_64                                                   17.03.1.ce-1.el7.centos                                                    docker-ce-stable
docker-ce.x86_64                                                   17.03.2.ce-1.el7.centos                                                    docker-ce-stable
docker-ce.x86_64                                                   17.03.3.ce-1.el7                                                           docker-ce-stable
docker-ce.x86_64                                                   17.06.0.ce-1.el7.centos                                                    docker-ce-stable
docker-ce.x86_64                                                   17.06.1.ce-1.el7.centos                                                    docker-ce-stable
docker-ce.x86_64                                                   17.06.2.ce-1.el7.centos                                                    docker-ce-stable
docker-ce.x86_64                                                   17.09.0.ce-1.el7.centos                                                    docker-ce-stable
docker-ce.x86_64                                                   17.09.1.ce-1.el7.centos                                                    docker-ce-stable
docker-ce.x86_64                                                   17.12.0.ce-1.el7.centos                                                    docker-ce-stable
docker-ce.x86_64                                                   17.12.1.ce-1.el7.centos                                                    docker-ce-stable
docker-ce.x86_64                                                   18.03.0.ce-1.el7.centos                                                    docker-ce-stable
docker-ce.x86_64                                                   18.03.1.ce-1.el7.centos                                                    docker-ce-stable
docker-ce.x86_64                                                   18.06.0.ce-3.el7                                                           docker-ce-stable
docker-ce.x86_64                                                   18.06.1.ce-3.el7                                                           docker-ce-stable
docker-ce.x86_64                                                   18.06.2.ce-3.el7                                                           docker-ce-stable
docker-ce.x86_64                                                   18.06.3.ce-3.el7                                                           docker-ce-stable
docker-ce.x86_64                                                   3:18.09.0-3.el7                                                            docker-ce-stable
docker-ce.x86_64                                                   3:18.09.1-3.el7                                                            docker-ce-stable
docker-ce.x86_64                                                   3:18.09.2-3.el7                                                            docker-ce-stable
# 安装docker 18.06版本
# yum install docker-ce-18.06.0.ce
```
重新执行kubeadm init
```shell
# kubeadm init --config kubeadm.yaml
[init] Using Kubernetes version: v1.13.3
[preflight] Running pre-flight checks
	[WARNING Hostname]: hostname "l-kubernetes-server1" could not be reached
	[WARNING Hostname]: hostname "l-kubernetes-server1": lookup l-kubernetes-server1 on 10.106.2.16:53: no such host
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [l-kubernetes-server1 localhost] and IPs [10.115.0.11 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [l-kubernetes-server1 localhost] and IPs [10.115.0.11 127.0.0.1 ::1]
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [l-kubernetes-server1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.115.0.11]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 21.002587 seconds
[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster
[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "l-kubernetes-server1" as an annotation
[mark-control-plane] Marking the node l-kubernetes-server1 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node l-kubernetes-server1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: xtzc6b.by1rget4rud8xyr7
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 10.115.0.11:6443 --token xtzc6b.by1rget4rud8xyr7 --discovery-token-ca-cert-hash sha256:e26362cb626b0f34c5c51fcc9bc751a30f49ed9c330a5559e5e6e02e502c8008
```
可以看见最开始版本未验证的warning消失了。  根据以上的提示信息，切换到普通用户，执行以下命令
```shell
To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
# 操作如下：
$ id xufeng2
uid=18592(xufeng2) gid=28004
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
#### 4.部署kubernetes的Worker节点
切换到server2上
先安装docker和kubeadm
```shell
# 先配置aliyun的docker-ce和kubernetes源（详见最上面）
yum install docker-ce-18.06.0.ce kubeadm
# 设置启动并开机启动
systemctl enable docker.service
systemctl start docker.service

systemctl enable kubelet.service
systemctl start kubelet.service
```
执行kubeadm join操作
```shell
# kubeadm join 10.115.0.11:6443 --token xtzc6b.by1rget4rud8xyr7 --discovery-token-ca-cert-hash sha256:e26362cb626b0f34c5c51fcc9bc751a30f49ed9c330a5559e5e6e02e502c8008
[preflight] Running pre-flight checks
[preflight] Some fatal errors occurred:
	[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
```
可以看到错误提示，显示bridge-nf-call-iptables不是0，那么去看下这个文件的内容
```shell
# cat /proc/sys/net/bridge/bridge-nf-call-iptables
0
# 将bridge-nf-call-iptables值设为1
# echo 1 > /proc/sys/net/bridge/bridge-nf-call-iptables
# 这样做是禁止iptables对bridge数据进行处理
```
然后再去执行join操作
```shell
# kubeadm join 10.115.0.11:6443 --token xtzc6b.by1rget4rud8xyr7 --discovery-token-ca-cert-hash sha256:e26362cb626b0f34c5c51fcc9bc751a30f49ed9c330a5559e5e6e02e502c8008
[preflight] Running pre-flight checks
[discovery] Trying to connect to API Server "10.115.0.11:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://10.115.0.11:6443"
[discovery] Requesting info from "https://10.115.0.11:6443" again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "10.115.0.11:6443"
[discovery] Successfully established connection with API Server "10.115.0.11:6443"
[join] Reading configuration from the cluster...
[join] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.13" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Activating the kubelet service
[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...
[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "l-kubernetes-server2" as an annotation

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the master to see this node join the cluster.
```
然后再去master上查看下node状态：
```shell
$ kubectl get nodes
NAME                                   STATUS     ROLES    AGE    VERSION
l-kubernetes-server1   NotReady   master   23h    v1.13.3
l-kubernetes-server2   NotReady   <none>   118s   v1.13.3
```
可以看到两个nodes的状态都是NotReady
在调试 Kubernetes 集群时，最重要的手段就是用kubelet describe查看这个node的信息、状态和事件。
```shell
$ kubectl describe node l-kubernetes-server1
...
Conditions:
...
Ready    False... 
reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
```
可以看到，NotReady的原因是因为没有部署网络插件。
当然也可以通过kubectl检查这个节点上各个pods的状态，其中kube-system是kubernetes项目预留的系统pod工作空间（Namespace，注意它并不是Linux Namespace，它只是kubernetes划分不同工作空间的单位）
```shell
$ kubectl get pods -n kube-system
# 或者使用 kubectl get pods --all-namespace
NAME                                                           READY   STATUS              RESTARTS   AGE
coredns-86c58d9df4-fqgw2                                        0/1     ContainerCreating   0          27h
coredns-86c58d9df4-m78mw                                        0/1     ContainerCreating   0          27h
etcd-l-kubernetes-server1                      					1/1     Running             1          27h
kube-apiserver-l-kubernetes-server1            					1/1     Running             1          27h
kube-controller-manager-l-kubernetes-server1   					1/1     Running             1          27h
kube-proxy-jwjz8                                                1/1     Running             0          27h
kube-proxy-t4k66                                                0/1     ContainerCreating   0          4h1m
kube-scheduler-l-kubernetes-server1
```
可以看到coredns是在等待网络插件的安装，所以在显示ContainerCreating状态。
#### 5.部署网络插件
在kubernetes项目中“一切皆容器”的设计理念指导下，部署网络插件很简单，只需要执行一句kubectl apply指令，以weave为例：
`$ kubectl apply -f https://git.io/weave-kube-1.6`
部署完成后，重新检查pods状态：
```shell
$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                                           READY   STATUS              RESTARTS   AGE
kube-system   coredns-86c58d9df4-fqgw2                                       0/1     ContainerCreating   0          44h
kube-system   coredns-86c58d9df4-m78mw                                       0/1     ContainerCreating   0          44h
kube-system   etcd-l-kubernetes-server1                      		     1/1     Running             1          44h
kube-system   kube-apiserver-l-kubernetes-server1            		     1/1     Running             1          44h
kube-system   kube-controller-manager-l-kubernetes-server1   	     	     1/1     Running             1          44h
kube-system   kube-proxy-jwjz8                                               1/1     Running             0          44h
kube-system   kube-proxy-t4k66                                               0/1     ContainerCreating   0          21h
kube-system   kube-scheduler-l-kubernetes-server1            	  	     1/1     Running             1          44h
kube-system   weave-net-fgc6l                                                2/2     Running             0          16h
kube-system   weave-net-jgcqv                                                0/2     ContainerCreating   0          16h
```
从上面可以看出，所有namespace中pod的状态。**server1为master，因为server2已经执行join操作，是worker节点，所以在控制节点master上安装插件会自动在所有节点上安装**。coredns、kube-proxy和weave-net均有两个pod，对别对应Master（server1）和Worker（server2）。
我们发现，网络插件已经安装完成了，但是还有很多ContainerCreating状态，那么就再查看pod的详细信息，此时看具体pod就需要指定namespace。
```shell
$ kubectl describe pod weave-net-jgcqv --namespace=kube-system
...
Events:
  Type     Reason                  Age                   From                                           Message
  ----     ------                  ----                  ----                                           -------
  Warning  FailedCreatePodSandBox  30m (x2128 over 17h)  kubelet, l-kubernetes-server2  Failed create pod sandbox: rpc error: code = Unknown desc = failed pulling image "k8s.gcr.io/pause:3.1": Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)

$ kubectl describe pod kube-proxy-t4k66 --namespace=kube-system
...
Events:
  Type     Reason                  Age                     From                                           Message
  ----     ------                  ----                    ----                                           -------
  Warning  FailedCreatePodSandBox  41m (x2654 over 21h)    kubelet, l-kubernetes-server2  Failed create pod sandbox: rpc error: code = Unknown desc = failed pulling image "k8s.gcr.io/pause:3.1": Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  FailedCreatePodSandBox  12m (x55 over 37m)      kubelet, l-kubernetes-server2  Failed create pod sandbox: rpc error: code = Unknown desc = failed pulling image "k8s.gcr.io/pause:3.1": Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Failed                  7m9s (x4 over 9m25s)    kubelet, l-kubernetes-server2  Error: ErrImagePull
  Normal   BackOff                 2m13s (x23 over 9m25s)  kubelet, l-kubernetes-server2  Back-off pulling image "k8s.gcr.io/kube-proxy:v1.13.3"
```
可以看到，信息来自server2，显示无法下载"k8s.gcr.io/pause:3.1‘‘’和“k8s.gcr.io/kube-proxy:v1.13.3"镜像，那么问题简单了，去自建仓库下载对应的镜像即可。下载完后会kubernetes会自动拉起容器，在查看下pod状态，都显示正在运行。
```shell
$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                                           READY   STATUS    RESTARTS   AGE
kube-system   coredns-86c58d9df4-fqgw2                                       1/1     Running   0          44h
kube-system   coredns-86c58d9df4-m78mw                                       1/1     Running   0          44h
kube-system   etcd-l-kubernetes-server1                      		     1/1     Running   1          44h
kube-system   kube-apiserver-l-kubernetes-server1            		     1/1     Running   1          44h
kube-system   kube-controller-manager-l-kubernetes-server1   	    	     1/1     Running   1          44h
kube-system   kube-proxy-jwjz8                                               1/1     Running   0          44h
kube-system   kube-proxy-t4k66                                               1/1     Running   0          21h
kube-system   kube-scheduler-l-kubernetes-server1            		     1/1     Running   1          44h
kube-system   weave-net-fgc6l                                                2/2     Running   0          17h
kube-system   weave-net-jgcqv                                                2/2     Running   8          17h
```
**注意：server2是worker节点，在执行join操作后，会需要从GCR中下载镜像（eg. k8s.gcr.io/pause k8s.gcr.io/kube-proxy等），但是国内网络是访问不了gcr.io的，所以以后在新节点执行join操作之前，一定要先从自建GCR仓库中下载镜像。**

再次查看各个node的状态：
```shell
$ kubectl get nodes
NAME                                   STATUS     ROLES    AGE   VERSION
l-kubernetes-server1   			Ready   master   45h   v1.13.3
l-kubernetes-server2   			Ready      <none>   22h   v1.13.3
```
#### 6.部署Dashboard可视化插件
最新版本详见[DashBoard](https://github.com/kubernetes/dashboard)
```shell
$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
$ kubectl apply -f kubernetes-dashboard.yaml
secret/kubernetes-dashboard-certs created
serviceaccount/kubernetes-dashboard created
role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created
deployment.apps/kubernetes-dashboard created
service/kubernetes-dashboard created
# 查看pod安装状态
$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                                           READY   STATUS             RESTARTS   AGE
kube-system   coredns-86c58d9df4-fqgw2                                       1/1     Running            0          3d19h
kube-system   coredns-86c58d9df4-m78mw                                       1/1     Running            0          3d19h
kube-system   etcd-l-kubernetes-server1.ops.dev.ten.wyj                      1/1     Running            1          3d19h
kube-system   kube-apiserver-l-kubernetes-server1.ops.dev.ten.wyj            1/1     Running            1          3d19h
kube-system   kube-controller-manager-l-kubernetes-server1.ops.dev.ten.wyj   1/1     Running            1          3d19h
kube-system   kube-proxy-jwjz8                                               1/1     Running            0          3d19h
kube-system   kube-proxy-t4k66                                               1/1     Running            0          2d20h
kube-system   kube-scheduler-l-kubernetes-server1.ops.dev.ten.wyj            1/1     Running            1          3d19h
kube-system   kubernetes-dashboard-57df4db6b-sptkp                           0/1     ImagePullBackOff   0          2m27s
kube-system   weave-net-fgc6l                                                2/2     Running            0          2d16h
kube-system   weave-net-jgcqv                                                2/2     Running            8          2d16h

# 查看pod状态信息
$ kubectl describe pod kubernetes-dashboard-57df4db6b-sptkp --namespace=kube-system
...
Events:
 Warning  Failed     15m (x4 over 17m)     kubelet, l-kubernetes-server2.ops.dev.ten.wyj  Failed to pull image "k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1": rpc error: code = Unknown desc = Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
...
```
可以看到，又是拉取镜像的问题，按照之前的方法解决。重新看下所有pod状态，运行成功。
```shell
$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                                           READY   STATUS    RESTARTS   AGE
kube-system   coredns-86c58d9df4-fqgw2                                       1/1     Running   0          3d20h
kube-system   coredns-86c58d9df4-m78mw                                       1/1     Running   0          3d20h
kube-system   etcd-l-kubernetes-server1.ops.dev.ten.wyj                      1/1     Running   1          3d20h
kube-system   kube-apiserver-l-kubernetes-server1.ops.dev.ten.wyj            1/1     Running   1          3d20h
kube-system   kube-controller-manager-l-kubernetes-server1.ops.dev.ten.wyj   1/1     Running   1          3d20h
kube-system   kube-proxy-jwjz8                                               1/1     Running   0          3d20h
kube-system   kube-proxy-t4k66                                               1/1     Running   0          2d21h
kube-system   kube-scheduler-l-kubernetes-server1.ops.dev.ten.wyj            1/1     Running   1          3d20h
kube-system   kubernetes-dashboard-57df4db6b-sptkp                           1/1     Running   0          38m
kube-system   weave-net-fgc6l                                                2/2     Running   0          2d17h
kube-system   weave-net-jgcqv                                                2/2     Running   8          2d17h
```
登录dashboard有两种方法：1.改默认端口8443端口到9090，使用http访问，并且免密登录。2.不改端口，添加NodePort，使用https+token。
**1.改默认端口8443端口到9090，使用http访问，并且免密登录**
修改之前下载的kubernetes-dashboard.yaml，将默认配置中的8443端口改成9090（原本镜像中就是有9090 非安全端口的，只是yaml文件没有暴露出来）
```shell
# 将8443 和 https 改成9090 和 http
# ------------------- Dashboard Deployment----------------
....
        ports:
        - containerPort: 9090
          protocol: TCP
        #- containerPort: 8443
        #  protocol: TCP
        args:
          #- --auto-generate-certificates
....
        livenessProbe:
          httpGet:
            scheme: HTTP
            #scheme: HTTPS
            path: /
            port: 9090
            #port: 8443
# ------------------- Dashboard Service -----------
...
spec:
  type: NodePort
  ports:
    - port: 9090
      targetPort: 9090
      nodePort: 30001
    #- port: 443
    #  targetPort: 8443
```
替换端口，并注释- --auto-generate-certificates，添加nodePort端口（添加映射到虚拟机的端口），然后`kubectl apply -f kubernetes-dashboard.yaml`更新配置，使用`http://10.115.0.11:30001`访问。
**2.不改端口，添加NodePort，使用https+token**
修改kubernetes-dashboard.yaml文件
```shell
# 在dashboard service中添加nodeport
spec:
  # type类型修改为NodePort
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
      # 添加映射到虚拟机的端口
      nodePort: 30001
```
修改完之后保存，使用`kubectl apply -f kubernetes-dashboard.yaml`重新加载配置，然后在浏览器中使用`https://ip:30001`登录。
1. 使用token登录。
```shell
# 查看所有命名空间下的所有secret
$ kubectl get secrets --all-namespaces
NAMESPACE     NAME                                             TYPE                                  DATA   AGE
default       default-token-mm9vz                              kubernetes.io/service-account-token   3      14d
kube-public   default-token-l4xvg                              kubernetes.io/service-account-token   3      14d
kube-system   attachdetach-controller-token-zt69t              kubernetes.io/service-account-token   3      14d
kube-system   bootstrap-signer-token-mm67w                     kubernetes.io/service-account-token   3      14d
kube-system   bootstrap-token-xtzc6b                           bootstrap.kubernetes.io/token         6      14d
kube-system   certificate-controller-token-hchjf               kubernetes.io/service-account-token   3      14d
kube-system   clusterrole-aggregation-controller-token-pd68p   kubernetes.io/service-account-token   3      14d
kube-system   coredns-token-x49sj                              kubernetes.io/service-account-token   3      14d
kube-system   cronjob-controller-token-c46zr                   kubernetes.io/service-account-token   3      14d
kube-system   daemon-set-controller-token-lddfz                kubernetes.io/service-account-token   3      14d
kube-system   default-token-nzhg7                              kubernetes.io/service-account-token   3      14d
kube-system   deployment-controller-token-tmlq6                kubernetes.io/service-account-token   3      14d
kube-system   disruption-controller-token-dcvpv                kubernetes.io/service-account-token   3      14d
kube-system   endpoint-controller-token-8pk7r                  kubernetes.io/service-account-token   3      14d
kube-system   expand-controller-token-6snt2                    kubernetes.io/service-account-token   3      14d
kube-system   generic-garbage-collector-token-snsnj            kubernetes.io/service-account-token   3      14d
kube-system   horizontal-pod-autoscaler-token-jxrd5            kubernetes.io/service-account-token   3      14d
kube-system   job-controller-token-fgfxn                       kubernetes.io/service-account-token   3      14d
kube-system   kube-proxy-token-kcft4                           kubernetes.io/service-account-token   3      14d
kube-system   kubernetes-dashboard-certs                       Opaque                                0      50m
kube-system   kubernetes-dashboard-key-holder                  Opaque                                2      11d
kube-system   kubernetes-dashboard-token-pfj2c                 kubernetes.io/service-account-token   3      50m
kube-system   namespace-controller-token-zsnlm                 kubernetes.io/service-account-token   3      14d
kube-system   node-controller-token-l25jp                      kubernetes.io/service-account-token   3      14d
kube-system   persistent-volume-binder-token-5tn2b             kubernetes.io/service-account-token   3      14d
kube-system   pod-garbage-collector-token-24bqz                kubernetes.io/service-account-token   3      14d
kube-system   pv-protection-controller-token-d2s4m             kubernetes.io/service-account-token   3      14d
kube-system   pvc-protection-controller-token-kqqqb            kubernetes.io/service-account-token   3      14d
kube-system   replicaset-controller-token-dl5n8                kubernetes.io/service-account-token   3      14d
kube-system   replication-controller-token-br8rx               kubernetes.io/service-account-token   3      14d
kube-system   resourcequota-controller-token-4phj4             kubernetes.io/service-account-token   3      14d
kube-system   service-account-controller-token-gcqzg           kubernetes.io/service-account-token   3      14d
kube-system   service-controller-token-cjr4l                   kubernetes.io/service-account-token   3      14d
kube-system   statefulset-controller-token-q4trj               kubernetes.io/service-account-token   3      14d
kube-system   token-cleaner-token-sl6cl                        kubernetes.io/service-account-token   3      14d
kube-system   ttl-controller-token-tsvqb                       kubernetes.io/service-account-token   3      14d
kube-system   weave-net-token-kk49j                            kubernetes.io/service-account-token   3      13d
```
只要是type为service-account-token的secret的token都可以使用。我们选择kubernetes-dashboard-token-pfj2c这个secret，使用其他secret会报权限不足问题。
```shell
$ kubectl describe secret kubernetes-dashboard-token-pfj2c -n kube-system
Name:         kubernetes-dashboard-token-pfj2c
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: kubernetes-dashboard
              kubernetes.io/service-account.uid: a7cbf60d-4630-11e9-a9ac-525400df2500

Type:  kubernetes.io/service-account-token

Data
====
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi1wZmoyYyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImE3Y2JmNjBkLTQ2MzAtMTFlOS1hOWFjLTUyNTQwMGRmMjUwMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.kDlRAJ_f2xuxax6aWindd_rHAHl24JnKy2E_Q89jwyvVBBskWTR-w3jDzc4KvNqyPfkgE2UgTiWQHUiGg17OAgLNu__guejWi70ErhEXzagPNUdtAqfj2A8i3b7vm96Ene0iAPKdNx-wjAU5Rnn4nvEfWustOeK-DokWdIZskeHb9-yT8DlxvowzkyPan5gbOhaiOQENESaBPZiNmPedZzgtroOuhQH9epwrmQtjxU-Omi_ryI4yOb676QS_AiF6qzyHcqtIPHfboWwj1eOGnTukXGpNobFr_kNhlXVi9NzFPmqDFvYb980aliAi1923iAIs-Q8ri9jFxc_jDJef7A
ca.crt:     1025 bytes
```
在登录界面中选择token登录，输入上方的token，就可以登录.
#### 7.部署容器存储插件
容器本身是无状态的，但是可以进行持久化存储。存储插件会在容器里挂载一个机遇网络或者其他机制的远程数据卷，使得在容器里创建的文件，实际上是保存在远程存储服务器上，或者以分布式的方式保存在多个节点上，而与当前宿主机没有任何绑定关系。这样，无论你在其他哪个宿主机上启动新的容器，都可以请求挂载指定的持久化存储卷，从而访问到数据卷里保存的内容，**这就是“持久化”的含义**。
Rook项目是一个基于Ceph的kubernetes存储插件。不过，不同于Ceph的简单封装，Rook在自己的实现中加入了水平扩展、迁移、灾难、备份、监控等大量的企业级的功能，使得Rook变成了一个完整的、生产级别可用的容器存储插件。
部署方法：
```shell
$ mkdir rook
$ wget https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml
$ wget https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml
$ kubectl create -f operator.yaml
$ kubectl create -f cluster.yaml
$ kubectl get pods --all-namespaces
NAMESPACE          NAME                                                           READY   STATUS    RESTARTS   AGE
kube-system        coredns-86c58d9df4-fqgw2                                       1/1     Running   0          16d
kube-system        coredns-86c58d9df4-m78mw                                       1/1     Running   0          16d
kube-system        etcd-l-kubernetes-server1.ops.dev.ten.wyj                      1/1     Running   1          16d
kube-system        kube-apiserver-l-kubernetes-server1.ops.dev.ten.wyj            1/1     Running   1          16d
kube-system        kube-controller-manager-l-kubernetes-server1.ops.dev.ten.wyj   1/1     Running   1          16d
kube-system        kube-proxy-jwjz8                                               1/1     Running   0          16d
kube-system        kube-proxy-t4k66                                               1/1     Running   0          15d
kube-system        kube-scheduler-l-kubernetes-server1.ops.dev.ten.wyj            1/1     Running   1          16d
kube-system        kubernetes-dashboard-57df4db6b-2hrnk                           1/1     Running   0          28h
kube-system        weave-net-fgc6l                                                2/2     Running   0          14d
kube-system        weave-net-jgcqv                                                2/2     Running   8          14d
rook-ceph-system   rook-ceph-agent-727qs                                          1/1     Running   0          113m
rook-ceph-system   rook-ceph-operator-b996864dd-qxl2z                             1/1     Running   0          114m
rook-ceph-system   rook-discover-tl24d                                            1/1     Running   0          113m
rook-ceph          rook-ceph-mgr-a-7cd9fbfbfd-j6zcl                               1/1     Running   0          108m
rook-ceph          rook-ceph-mon-a-79b9c8f694-bjglw                               1/1     Running   0          109m
rook-ceph          rook-ceph-mon-b-54888c948b-q7j77                               1/1     Running   0          109m
rook-ceph          rook-ceph-mon-c-6b59cd89cd-pwbmh                               1/1     Running   0          109m
```
可以看到，多了两个namespace：rook-ceph-system和rook-ceph。这样，一个基于Rook持久化存储集群就以容器的方式运行起来，接下来kubernetes项目上创建的所有Pod就能通过Persistent Volume（PV）和Persistent Volume Claim（PVC）的方式，在容器里挂载由Ceph提供的数据卷了。